#!/opt/venv/bin/python
import sys
import subprocess
from llama_cpp import Llama

# ğŸ§  GPT initialisieren
llm = Llama(model_path="/opt/phi-2.Q4_K_M.gguf", n_ctx=2048, n_gpu_layers=-1)

# ğŸ”Š TTS Antwort ausgeben
def tts_ausgabe(text):
    print(f"ğŸ—£ï¸ TTS-Ausgabe: {text}")
    subprocess.run(["/opt/venv/bin/python", "/opt/script/coqui_tts.py", text])

# âœ… Eingabetext prÃ¼fen
if len(sys.argv) < 2:
    print("âŒ Bitte gib einen Test-Text ein.")
    sys.exit(1)

test_text = sys.argv[1]
print(f"ğŸ“ Test-Text: {test_text}")

# ğŸ§  GPT Antwort generieren
prompt = f"Antworte freundlich auf Deutsch auf: {test_text}\n"
output = llm(prompt, max_tokens=100, stop=["\n", "User:"], echo=False)
antwort = output['choices'][0]['text'].strip()
print(f"ğŸ§  GPT-Antwort: {antwort}")

# ğŸ”Š Antwort ausgeben
tts_ausgabe(antwort)
